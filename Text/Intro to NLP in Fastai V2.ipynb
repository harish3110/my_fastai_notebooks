{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) [Path('README'),Path('tmp_lm'),Path('tmp_clas'),Path('imdb.vocab'),Path('train'),Path('test'),Path('unsup')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#100000) [Path('train/neg/4336_4.txt'),Path('train/neg/10833_3.txt'),Path('train/neg/4851_3.txt'),Path('train/neg/4602_3.txt'),Path('train/neg/10929_1.txt'),Path('train/neg/1183_2.txt'),Path('train/neg/6287_4.txt'),Path('train/neg/1907_2.txt'),Path('train/neg/7652_3.txt'),Path('train/neg/11370_1.txt')...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup']) #folders restricts the function from only getting files form those folders\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I watched this movie and the original Carlitos Way back to back. The difference between the two is disgusting. Now i know that people are going to say that the prequel was made on a small budget but that never had anything to do with a bad script. Now maybe it's just me, but i always thought that a prequel was made to go set up the other movie, starring key characters and maybe filling in a bit about life that we didn't know. Rise to Power is just a movie that has Carlito's name. There should have been at least a few characters from the original movie, the ending makes no sense in relation to the original. In the end of this movie he retires with his sweet heart but how the hell do we get him coming out of prison in the next movie? And his woman isn't even the same woman that he talks about as his only love in the original. I would say the movie is mildly entertaining in its self, with a few decent bits but it pales when held up to it's big brother. Don't lay awake at night waiting to see this, watch the original one more time if you really need a hit.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read()\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Fastai's default tokenizer: Spacy\n",
    "# Two methods available: WordTokenizer and SpacyTokenizer\n",
    "\n",
    "spacy = WordTokenizer() # WordTokenizer will point out to the latest tokenizer being used in Fastai, need not always be Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#233) ['I','watched','this','movie','and','the','original','Carlitos','Way','back','to','back','.','The','difference','between','the','two','is','disgusting','.','Now','i','know','that','people','are','going','to','say'...]\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = first(spacy([txt]))\n",
    "coll_repr(toks, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai's Tokenizer Class\n",
    "\n",
    "Small modifications on Spacy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#246) ['xxbos','i','watched','this','movie','and','the','original','xxmaj','carlitos','xxmaj','way','back','to','back','.','xxmaj','the','difference','between','the','two','is','disgusting','.','xxmaj','now','i','know','that','people'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are now some tokens added that start with the characters \"xx\", which is not a common word prefix in English. These are special tokens.\n",
    "- \"xxbos\", is a special token that indicates the start of a new text (\"BOS\" is a standard NLP acronym which means \"beginning of stream\"). \n",
    "- These special tokens are added in Fastai library on top of the Spacy. \n",
    "\n",
    "\n",
    "> For instance, the rules will replace a sequence of four exclamation points with a single exclamation point, followed by a special repeated character token, and then the number four. In this way, the model's embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalised word will be replaced with a special capitalisation token, followed by the lower case version of the word. This way, the embedding matrix only needs the lower case version of the words, saving compute and memory, but can still learn the concept of capitalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the rules that were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai2.text.core.fix_html(x)>,\n",
       " <function fastai2.text.core.replace_rep(t)>,\n",
       " <function fastai2.text.core.replace_wrep(t)>,\n",
       " <function fastai2.text.core.spec_add_spaces(t)>,\n",
       " <function fastai2.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai2.text.core.replace_all_caps(t)>,\n",
       " <function fastai2.text.core.replace_maj(t)>,\n",
       " <function fastai2.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.7/site-packages/fastai2/text/core.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "replace_rep??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief summary of what each does:\n",
    "\n",
    "- `fix_html`:: replace special HTML characters by a readable version (IMDb reviwes have quite a few of them for instance) ;\n",
    "- `replace_rep`:: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it's repeated, then the character ;\n",
    "- `replace_wrep`:: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it's repeated, then the word ;\n",
    "- `spec_add_spaces`:: add spaces around / and # ;\n",
    "- `rm_useless_spaces`:: remove all repetitions of the space character ;\n",
    "- `replace_all_caps`:: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\n",
    "- `replace_maj`:: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\n",
    "- `lowercase`:: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#3) ['xxbos','xxmaj','fast.ai']\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn('Fast.ai'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the word tokenization approach seen in the last section, another popular tokenization method is subword tokenization. Word tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a sentence. However, this assumption is not always appropriate. For instance, consider this sentence: 我的名字是郝杰瑞 (which means \"My name is Jeremy Howard\" in Chinese). That's not going to work very well with a word tokenizer, because there are no spaces in it! Languages like Chinese and Japanese don't use spaces, and in fact they don't even have a well-defined concept of a \"word\". There are also languages, like Turkish and Hungarian, which can add many bits together without spaces, to create very long words which include a lot of separate pieces of information.\n",
    "\n",
    "To handle these cases, it's generally best to use subword tokenization. This proceeds in two steps:\n",
    "\n",
    "1. Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.\n",
    "2. Tokenize the corpus using this vocab of *subword units*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets look at a corpus of 2000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2000) [\"I watched this movie and the original Carlitos Way back to back. The difference between the two is disgusting. Now i know that people are going to say that the prequel was made on a small budget but that never had anything to do with a bad script. Now maybe it's just me, but i always thought that a prequel was made to go set up the other movie, starring key characters and maybe filling in a bit about life that we didn't know. Rise to Power is just a movie that has Carlito's name. There should have been at least a few characters from the original movie, the ending makes no sense in relation to the original. In the end of this movie he retires with his sweet heart but how the hell do we get him coming out of prison in the next movie? And his woman isn't even the same woman that he talks about as his only love in the original. I would say the movie is mildly entertaining in its self, with a few decent bits but it pales when held up to it's big brother. Don't lay awake at night waiting to see this, watch the original one more time if you really need a hit.\",\"I wanted to like Magnolia. The plot reminded me of Grand Canyon (which I liked). 4 different lives/stories that come together at the end but Magnolia took a wrong turn halfway through the movie and I was lost. I almost turned it off right then and there but I felt I should hang in there until the end, little did I know it would be another torturous 1 1/2 hours. Thank god I rented instead of seeing it in the theatre. I almost screamed out in frustration after 2 hours. The biggest kick in the pants was the ending frog scene. My DVD player still hasn't forgiven me and I don't blame it one bit. It was a unique movie, but a bad, boring, and pointless movie.\",'watch a team of bomb disposal experts in Iraq count down their time before they can go home.<br /><br />That in itself sounds boring. Every time that little caption came up telling us how long they had left, it just caused this film with no plot to drag on and on. hurry up and finish your time there so we can all go home.<br /><br />I must be missing something. I\\'m a great fan of war films if they are done well. This had \"jarhead\" syndrome. A film that at times was beautifully shot, but cinematography doesn\\'t stop it from being totally dull and pointless.<br /><br />And get over the slow mo \"cartridges coming out of the gun\" shot already. they could have saved money and just got stock footage from any other film with a gun in it.<br /><br />I didn\\'t have any empathy for the main guy in it, i was constantly hoping that his recklessness would cause him to die. In fact the film would have worked much better if he had.<br /><br />I read some reviews and seemed to get the feeling that those who had been in the armed forces disliked it, and everyone else loved it. I have never been in the forces, and I\\'m with them. It\\'s pretentious drivel. the 3 stars are for the cinematography.','A film without conscience. Drifter agrees to kill a man for a mobster for money. Then they double cross him. Meanwhile he falls in love with the dead man\\'s wife, and, without her knowing he\\'s the killer, moves in with her. Then he \"accidentally\" kills her when she finds out. Then, in a WALKING TALL kind of heroism, he gets revenge on the mobsters who double crossed him. The first problem is that, by agreeing to take on the murder by hire assignment, the drifter loses all sense of sympathy, worthiness, and heroism. We can\\'t accept any goodness in him and as a result the rest of the has no moral center. We just can\\'t care about that kind of guy. And the wife (nicely played by the fetching Kari Wuhrer - the sheriff in EIGHT LEGGED FREAKS), a high class lady who runs a mission for homeless people, similarly loses a degree of sympathy by jumping right into bed with the homeless drifter (despite her evidently weakened state after the death of her husband). And, when she finds out he\\'s the guy \\x96 what does she do? She locks him inside her house (as if ALL houses had locks you can\\'t open from INSIDE) with her and proceeds to berate him. Stoo-pid. George Wendt, however, is terrific in a role as a beefy thug. Director Stuart Gordon did so much better with RE-ANIMATOR and DAGON.',\"This movie was the beatliest mormon movie made yet. It made the RM & Sons of Provo look like well done films! It was supposed to be funny from what I was told. The best part was the best actor in the movie-Travis Eberhard-if he wasn't in the movie it probably wouldn't have been made! He ruled!<br /><br />10. It wasn't funny 9. It was beat 8. It had Thurl Big T Bailey, who's character made no sense 7. It was made in Provo 6. It didn't make fun of Brokeback 5. It had Larry H. Miller in it 4. It was the 1st movie Clint Howard wasn't funny in 3. Gary Coleman chose the perfect movie 4 a comeback 2. They should have cast at Surreal Life auditions 1. It was made by Halestorm Entertainment!!\",'This, and Immoral Tales, both left a bad taste in my mouth. It seems to me that Borowczyk is disgusted by sex, and these two films are cautionary tales about what will happen if you do have sex. As a film, it\\'s not very well done -- some of the acting is truly epically bad (such as the \"American\" woman with the French accent). The young woman\\'s sudden flip-flop from being anxious about the marriage to being interested (when it seems like it should have been the other way around), and the aunt\\'s sudden realization of the young man\\'s secret don\\'t make sense -- they\\'re not explained at all. I also didn\\'t like how the daughter\\'s relationship with a black man was presented as a sign of her family\\'s perversion or predilection for bestiality. The central idea, the idea that there\\'s this \"sexy beast,\" if you will, that lives in the woods, could have been a foundation for a perverse but fun story, but instead is just used as a basis for a nasty, sex-negative, morality play.','I read about this movie in a magazine and I was intrigued. A woman, who one day sees herself drive past in her own car. Well, I thought, this could be interesting...<br /><br />...but it isn\\'t. First, the title. The Broken? The Broken...what? What is broken? The...oh, wait...I get it, the title itself is \"broken\"! WOW, clever! Unfortunately, this is virtually the only thing going for it.<br /><br />The premise is not that bad, but I think Kiefer Suderland did much better in \\'Mirrors\\'. A cross between Invasion of the Body Snatchers and Mirrors, and a rather mediocre one at that. A more suited title would be \\'The Boring\\', since it draws out every single scene for bloody ages. Or maybe \\'The Confusing\\' since it doesn\\'t explain anything at all, not in the narrative nor in the story itself, only some vague idea about evil copies and somesuch, dotted with cheap scares and scenes used to death, but nothing tangible. It\\'s just messed up.<br /><br />On the other hand, the acting and the special effects are quite good, but then again, it\\'s not a difficult role to act.<br /><br />After watching the movie twice, I still feel unsatisfied, a little confused maybe, and not in the E. A. Poe or Stephen King kind of way. Do yourself a favor, and don\\'t watch this one. Simply put, there are better thrillers out there.','This horrendously bad piece of trash manages to be racist, sexist and homophobic all at once, while pretending to be terribly chic and sophisticated. Atrocious performances, a cliche ridden screenplay, and boring direction make this movie one to steer clear of. Two scenes were especially offensive - the one in which Schaech scrubs his tongue after being kissed by another man (could it really have been that gross), and the scene where Eastwood is kissed by Schaech\\'s best friend, who is pretending to be Russian. After he leaves the room she exclaims \"f**king foreigners\"! So much for her being a cultured artist who dreams of living in Paris!?!<br /><br />Jonathon Schaech can be a likeable actor on screen, and is astonishingly good-looking. It\\'s a shame he didn\\'t learn more from working with cutting edge gay director Gregg Araki on an earlier film, and try to salvage this film from descending into a string of gay stereotypes and a mire of homophobia.','In 1967, mine workers find the remnants of an ancient vanished civilization named Abkani that believe there are the worlds of light and darkness. When they opened the gate between these worlds ten thousand years ago, something evil slipped through before the gate was closed. Twenty-two years ago, the Government Paranormal Research Agency Bureau 713 was directed by Professor Lionel Hudgens (Matthew Walker), who performed experiments with orphan children. On the present days, one of these children is the paranormal investigator Edward Carnby (Christian Slater), who has just gotten an Abkani artifact in South America, and is chased by a man with abilities. When an old friend of foster house disappears in the middle of the night, he discloses that demons are coming back to Earth. With the support of the anthropologist Aline Cedrac (Tara Reid) and the leader of the Bureau 713, Cmdr. Richard Burke (Stephen Dorff), and his squad, they battle against the evil creatures.<br /><br />In spite of having a charismatic good cast, leaded by Christian Slater, Tara Reid and Stephen Dorff, \"Alone in the Dark\" never works and is a complete mess, without development of characters or plot. The reason may be explained by the \"brilliant\" interview of director Uwe Boll in the Extras of the DVD, where he says that \"videogames are the bestsellers of the younger generations that are not driven by books anymore\". Further, his target audience would be people aged between twelve and twenty-five years old. Sorry, but I find both assertions disrespectful with the younger generations. I have a daughter and a son, and I know many of their friends and they are not that type of stupid stereotype the director says. Further, IMDb provides excellent statistics to show that Mr. Uwe Boll is absolutely wrong. My vote is three.<br /><br />Title (Brazil): \"Alone in the Dark \\x96 O Despertar do Mal\" (\"Alone in the Dark \\x96 The Awakening of the Evil\")',\"This video contains an outsmart way to confuse and manipulate Americans about Islam. It's a pity that the people who did it really believe that American people is so dumb to believe in it, perhaps, as an American citizen, every person must protest against this kind of crap. If you want to know the truth about Islam, don't let nobody tell you... THE QURAN IS PUBLIC! you can read it by yourself and decide if what they say it's true or false...<br /><br />The video uses a lot of audiovisual strategies directed to manipulate and associate things that are not even related. The music used at some points prepare the public to hate what they see, even if they don't really understand what's going on in there. They use images that are misplaced from their original content.<br /><br />To end the comment I would like to make a reflexion... Don't you think you can do the same exact movie with every religion in the world?\"...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = L(o.open().read() for o in files[:2000])\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sz` ndicates the size of the embeddings which we want to pass. It determines how the subword tokens are formed. \n",
    "\n",
    "- A small size indicates that the subwords repeating are less and thus require many tokens to represent the sentence. \n",
    "\n",
    "- Whereas a large size would mean that entire common words in the text become tokens. \n",
    "\n",
    "Thus, finding the exact size is a trial and error method to get a good model. Picking a subword vocab size represents a compromise: a larger vocab means more fewer tokens per sentence, which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Overall, subword tokenization provides a way to easily scale between character tokenization (i.e. use a small subword vocab) and word tokenization (i.e. use a large subword vocab), and handles every human language without needing language-specific algorithms to be developed. It can even handle other \"languages\" such as genomic sequences or MIDI music notation! For this reason, in the last year its popularity has soared, and it seems likely to become the most common tokenization approach (it may well already be, by the time you read this!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our texts as tokens, next step is to numericalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization with Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#246) ['xxbos','i','watched','this','movie','and','the','original','xxmaj','carlitos','xxmaj','way','back','to','back','.','xxmaj','the','difference','between','the','two','is','disgusting','.','xxmaj','now','i','know','that','people'...]\n"
     ]
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#246) ['xxbos','i','watched','this','movie','and','the','original','xxmaj','carlitos'...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = corpus[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#1912) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','to','of','is','i','it','this'...]\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   17,  281,   19,   27,   13,    9,  203,    8,    0,    8,  130,\n",
       "         177,   14,  177,   10,    8,    9, 1403,  263])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos i watched this movie and the original xxmaj xxunk xxmaj way back to back . xxmaj the difference between'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a text Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model using DataBlock\n",
    "\n",
    "fastai handles tokenization and numericalization automatically when `TextBlock` is passed to `DataBlock`. All of the arguments that can be passed to `Tokenize` and `Numericalize` can also be passed to `TextBlock`. In the next chapter we'll discuss the easiest ways to run each of these steps separately, to ease debugging--but you can always just debug by running them manually on a subset of your data as shown in the previous sections. And don't forget about `DataBlock`'s handy `summary` method, which is very useful for debugging data issues.\n",
    "\n",
    "Here's how we use `TextBlock` to create a language model, using fastai's defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that's different to previous types used in `DataBlock` is that we're not just using the class directly (i.e. `TextBlock(...)`, but instead are calling a *class method*. A class method is a Python method which, as the name suggests, belongs to a *class* rather than an *object*. (Be sure to search online for more information about class methods if you're not familiar with them, since they're commonly used in many Python libraries and applications; we've used them a few times previously in the book, but haven't called attention to them.) The reason that `TextBlock` is special is that setting up the numericalizer's vocab can take a long time (we have to read every document and tokenize it to get the vocab); to be as efficient as possible fastai does things such as: \n",
    "\n",
    "- Save the tokenized documents in a temporary folder, so fastai doesn't have to tokenize more than once\n",
    "- Runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs.\n",
    "\n",
    "Therefore we need to tell `TextBlock` how to access the texts, so that it can do this initial preprocessing--that's what `from_folder` does.\n",
    "\n",
    "`show_batch` then works in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i really hope that xxmaj concorde / xxmaj new xxmaj horizons was n't trying to make a serious horror , or even action movie when they made xxmaj carnosaur 3 . xxmaj the movie is flat - out silly from start to finish . xxmaj even the humor in xxup xxunk is funny because it 's bad . xxmaj definitely a high water mark in the ' so xxmaj bad it 's xxmaj good ' genre . xxmaj if</td>\n",
       "      <td>i really hope that xxmaj concorde / xxmaj new xxmaj horizons was n't trying to make a serious horror , or even action movie when they made xxmaj carnosaur 3 . xxmaj the movie is flat - out silly from start to finish . xxmaj even the humor in xxup xxunk is funny because it 's bad . xxmaj definitely a high water mark in the ' so xxmaj bad it 's xxmaj good ' genre . xxmaj if you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at the xxmaj film xxmaj forum , recently , i could not resist watching this masterpiece once more when it was shown by xxup tcm , the other night . \\n\\n xxmaj this movie owes a debt of gratitude to xxmaj graham xxmaj greene , a writer who had the most developed sense of intrigue among his contemporaries and one of the best writers of the last century . xxmaj it also helped that a great director , xxmaj carol</td>\n",
       "      <td>the xxmaj film xxmaj forum , recently , i could not resist watching this masterpiece once more when it was shown by xxup tcm , the other night . \\n\\n xxmaj this movie owes a debt of gratitude to xxmaj graham xxmaj greene , a writer who had the most developed sense of intrigue among his contemporaries and one of the best writers of the last century . xxmaj it also helped that a great director , xxmaj carol xxmaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For converting the integer word indices into activations that we can use for our neural network, we will use embeddings, just like we did for collaborative filtering and tabular modelling. Then those embeddings are fed in a *Recurrent Neural Network* (RNN), using an architecture called *AWD_LSTM* (we will show how to write such a model from scratch in <<chapter_nlp_dive>>). As we discussed earlier, the embeddings in the pretrained model are merged with random embeddings added for words that weren't in the pretraining vocabulary. This is handled automatically inside `language_model_learner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function used by default is cross entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). A metric often used in NLP for language models is called *perplexity*. It is the exponential of the loss (i.e. `torch.exp(cross_entropy)`). We  will also add accuracy, to see how many times our model is right when trying to predict the next word, since cross entropy (as we've seen) is both hard to interpret, and also tells you more about the model's confidence, rather than just its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.104637</td>\n",
       "      <td>3.908968</td>\n",
       "      <td>0.299777</td>\n",
       "      <td>49.847504</td>\n",
       "      <td>26:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1037' class='' max='2629', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      39.44% [1037/2629 10:43<16:28 3.9338]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"I liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
    "         for _ in range(N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the classifier dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `DataBlock` definition above, every piece is familiar from previous data blocks we've built, with two important exceptions:\n",
    "\n",
    "- `TextBlock.from_folder` no longer has the `is_lm=True` parameter, and\n",
    "- We pass the `vocab` we created for the language model fine-tuning.\n",
    "\n",
    "The reason that we pass the vocab of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won't make any sense to this model, and the fine-tuning step won't be of any use.\n",
    "\n",
    "By passing `is_lm=False` (or not passing `is_lm` at all, since it defaults to `False`) we tell `TextBlock` that we have regular labeled data, rather than using the next tokens as labels. There is one challenge we have to deal with, however, which is to do with collating multiple documents into a minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, PyTorch `DataLoader`s need to collate all the items in a batch into a single tensor, and that a single tensor has a fixed shape (i.e. it has some particular length on every axis, and all items must be consistent). This should look a bit familiar: we had the same issue with images. In that case, we use cropping, padding, and/or squishing to make everything the same size. Cropping might not be a good idea for documents, because it seems likely we'd remove some key information (having said that, the same issue is true for images, and we use cropping there; data augmentation hasn't been well explored for NLP yet, so perhaps there are actually opportunities to use cropping in NLP too!) You can't really \"squish\" a document. So that leaves padding!\n",
    "\n",
    "We will expand the shortest texts to make them all the same size. To do this, we use a special token that will be ignored by our model. This is called *padding* (just like in vision). Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend of be of similar lengths. We won't make every batch, therefore, the same size, but will instead use the size of the largest document in each batch. (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, although as we write these words, no library provides good support for this yet, and there aren't any papers covering it. It's something we're planning to add to fastai soon however, so have a look on the book website, where we'll add information about this if and when it's working well.)\n",
    "\n",
    "The padding and sorting is automatically done by the data block API for us when using a `TextBlock`, with `is_lm=False`. (We don't have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)\n",
    "\n",
    "We can now create a model to classify our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the previously learnt languagle model fine tuned as the encoder and use this learner as the decoder which classifes the text. \n",
    "\n",
    "We use `load_encoder` instead of `load` because we only have pretrained weights available for the encoder; `load` by default raises an exception if an incomplete model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to train with discriminative learning rates and **gradual unfreezing**. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass -2 to freeze_to to freeze all except the last two parameter groups:\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze a bit more and train further\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze the entire model and train\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach 94.3% accuracy, which was state-of-the-art just three years ago. By training a model on all the texts read backwards and averaging the predictions of those two models, we can even get to 95.1% accuracy, which was the state of the art introduced by the [ULMFiT paper](https://arxiv.org/abs/1801.06146). It was only beaten a few months ago, fine-tuning a much bigger model and using expensive data augmentation (translating sentences in another language and back, using another model for translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
